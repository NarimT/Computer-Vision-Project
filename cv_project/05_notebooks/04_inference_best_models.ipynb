{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FryMURcPLgq1",
        "outputId": "14b703f2-b452-4485-b08e-a12d7270ba8f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sahi --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv8LijAYLr-e",
        "outputId": "304708ae-940f-403e-f96b-880f60193dbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sahi ultralytics rasterio geopandas shapely opencv-python-headless --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kumW9mF2MpV6",
        "outputId": "b0bb6d31-2bfd-4ac2-914b-6268a4e4f436"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Importaciones de SAHI\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from sahi.utils.cv import visualize_object_predictions"
      ],
      "metadata": {
        "id": "AHZlyQVeLZeS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMtlC2nfKs4f",
        "outputId": "0c7182c9-26cb-46dd-84d7-35ce2c080d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Device: CUDA (GPU)\n",
            "‚è≥ Leyendo GeoTIFF con Rasterio para asegurar bandas 2, 3, 1 y escalando a 8-bit...\n",
            "‚è≥ Cargando modelo YOLOv8 desde yolo_best.pt...\n",
            "‚è≥ Iniciando inferencia por Tiles con array de p√≠xeles...\n",
            "Performing prediction on 209 slices.\n",
            "‚úÖ Inferencia completada. Palmas detectadas: 18922\n",
            "üíæ Imagen visual (PNG) guardada en: final_results/visual_map_centroids.png\n",
            "üíæ GeoJSON guardado en: final_results/detected_palms.geojson\n",
            "\n",
            "üöÄ PROCESO FINALIZADO CON √âXITO\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# ‚öôÔ∏è CONFIGURATION\n",
        "# ==========================================\n",
        "# Aseg√∫rate de que este archivo exista\n",
        "IMAGE_PATH = \"Sample-drone-data-copy.tif\"\n",
        "MODEL_PATH = \"yolo_best.pt\"\n",
        "OUTPUT_DIR = \"final_results\"\n",
        "\n",
        "# INFERENCE SETTINGS\n",
        "SLICE_SIZE = 640         # Must match your training size\n",
        "OVERLAP_RATIO = 0.2      # 20% overlap\n",
        "CONFIDENCE = 0.25        # 25% minimum confidence\n",
        "IOU_THRESHOLD = 0.5      # 50% overlap threshold (NMS)\n",
        "# SLICE_SIZE = 640        # Tama√±o del tile (est√°ndar para YOLOv8)\n",
        "# OVERLAP_RATIO = 0.2     # 20% de solapamiento\n",
        "# CONFIDENCE = 0.65       # Umbral de confianza\n",
        "# IOU_THRESHOLD = 0.3     # NMS para eliminar duplicados\n",
        "\n",
        "# VISUALIZATION SETTINGS\n",
        "DOT_SIZE = 10\n",
        "DOT_COLOR = (0, 0, 255) # Rojo (BGR)\n",
        "TEXT_COLOR = (255, 255, 255)\n",
        "BG_COLOR = (0, 0, 0)\n",
        "\n",
        "# ==========================================\n",
        "# üöÄ SMART DEVICE DETECTION\n",
        "# ==========================================\n",
        "def get_optimal_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"‚úÖ Device: CUDA (GPU)\")\n",
        "        return 'cuda:0'\n",
        "    elif torch.backends.mps.is_available():\n",
        "        print(\"‚úÖ Device: MPS (Mac M1/M2)\")\n",
        "        return 'mps'\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Device: CPU (Lento)\")\n",
        "        return 'cpu'\n",
        "\n",
        "# ==========================================\n",
        "# üõ†Ô∏è MAIN EXECUTION\n",
        "# ==========================================\n",
        "def main():\n",
        "    # 0. Validaciones previas\n",
        "    if not os.path.exists(IMAGE_PATH):\n",
        "        sys.exit(f\"‚ùå Error: No se encuentra la imagen: {IMAGE_PATH}\")\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        sys.exit(f\"‚ùå Error: No se encuentra el modelo: {MODEL_PATH}\")\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    device = get_optimal_device()\n",
        "\n",
        "    # --------------------------------------\n",
        "    # 1. CARGAR RASTERIO, PREPARAR ARRAY Y CONVERTIR 16-BIT A 8-BIT\n",
        "    # --------------------------------------\n",
        "    print(\"‚è≥ Leyendo GeoTIFF con Rasterio para asegurar bandas 2, 3, 1 y escalando a 8-bit...\")\n",
        "    with rasterio.open(IMAGE_PATH) as src:\n",
        "        transform = src.transform\n",
        "        crs = src.crs\n",
        "\n",
        "        # Leemos bandas 2, 3, y 1 en ese orden\n",
        "        # Formato inicial: (3, H, W)\n",
        "        img_array = src.read([2, 3, 1])\n",
        "\n",
        "        # FIX CRUCIAL: Escalar de 16-bit (uint16) a 8-bit (uint8) para compatibilidad con YOLO/CV\n",
        "        # Si ya es uint8, esto no har√° nada. Si es uint16, lo normaliza.\n",
        "        max_val_in_data = np.max(img_array)\n",
        "        if max_val_in_data > 255:\n",
        "            # Escalar y convertir a float temporalmente\n",
        "            img_scaled = img_array.astype(np.float32) * (255.0 / max_val_in_data)\n",
        "            # Convertir al tipo final (uint8)\n",
        "            img_final_uint8 = img_scaled.astype(np.uint8)\n",
        "        else:\n",
        "            # Si ya est√° en rango 0-255, solo convertimos a uint8\n",
        "            img_final_uint8 = img_array.astype(np.uint8)\n",
        "\n",
        "        # Transponer para YOLO/SAHI: (Alto, Ancho, Canales) -> (H, W, 3)\n",
        "        image_for_sahi = np.transpose(img_final_uint8, (1, 2, 0))\n",
        "\n",
        "        # image_cv (BGR) es para dibujar con OpenCV\n",
        "        image_cv = cv2.cvtColor(image_for_sahi, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # --------------------------------------\n",
        "    # 2. CARGAR MODELO (SAHI WRAPPER)\n",
        "    # --------------------------------------\n",
        "    print(f\"‚è≥ Cargando modelo YOLOv8 desde {MODEL_PATH}...\")\n",
        "    try:\n",
        "        detection_model = AutoDetectionModel.from_pretrained(\n",
        "            model_type='yolov8',\n",
        "            model_path=MODEL_PATH,\n",
        "            confidence_threshold=CONFIDENCE,\n",
        "            device=device\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Si falla aqu√≠, el modelo .pt est√° da√±ado.\n",
        "        sys.exit(f\"‚ùå Error cargando modelo SAHI (Revisa el archivo .pt): {e}\")\n",
        "\n",
        "    # --------------------------------------\n",
        "    # 3. INFERENCIA CON SAHI (SLICING)\n",
        "    # --------------------------------------\n",
        "    print(\"‚è≥ Iniciando inferencia por Tiles con array de p√≠xeles...\")\n",
        "\n",
        "    # Pasamos el array de NumPy, NO la ruta, para evitar el error de lectura de TIFF.\n",
        "    result = get_sliced_prediction(\n",
        "        image_for_sahi, # <--- Array 8-bit listo para YOLO\n",
        "        detection_model,\n",
        "        slice_height=SLICE_SIZE,\n",
        "        slice_width=SLICE_SIZE,\n",
        "        overlap_height_ratio=OVERLAP_RATIO,\n",
        "        overlap_width_ratio=OVERLAP_RATIO,\n",
        "        postprocess_type=\"NMS\",\n",
        "        postprocess_match_metric=\"IOU\",\n",
        "        postprocess_match_threshold=IOU_THRESHOLD,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    total_trees = len(result.object_prediction_list)\n",
        "    print(f\"‚úÖ Inferencia completada. Palmas detectadas: {total_trees}\")\n",
        "\n",
        "    # --------------------------------------\n",
        "    # 4. DIBUJAR CENTROIDES (VISUAL)\n",
        "    # --------------------------------------\n",
        "    # ... (El c√≥digo de dibujo)\n",
        "    for pred in result.object_prediction_list:\n",
        "        bbox = pred.bbox\n",
        "        center_x = int((bbox.minx + bbox.maxx) / 2)\n",
        "        center_y = int((bbox.miny + bbox.maxy) / 2)\n",
        "        cv2.circle(image_cv, (center_x, center_y), DOT_SIZE, DOT_COLOR, -1)\n",
        "\n",
        "    # Etiqueta tipo Dashboard\n",
        "    label_text = f\" Palms: {total_trees} \"\n",
        "    cv2.rectangle(image_cv, (0, 0), (350, 60), BG_COLOR, -1)\n",
        "    cv2.putText(image_cv, label_text, (10, 45),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, TEXT_COLOR, 3)\n",
        "\n",
        "    # Guardar PNG\n",
        "    out_png = os.path.join(OUTPUT_DIR, \"visual_map_centroids.png\")\n",
        "    cv2.imwrite(out_png, image_cv)\n",
        "    print(f\"üíæ Imagen visual (PNG) guardada en: {out_png}\")\n",
        "\n",
        "    # --------------------------------------\n",
        "    # 5. EXPORTAR GEOJSON (GEO-REFERENCIADO)\n",
        "    # --------------------------------------\n",
        "    records = []\n",
        "\n",
        "    for i, pred in enumerate(result.object_prediction_list):\n",
        "        bbox = pred.bbox\n",
        "        score = pred.score.value\n",
        "        px = (bbox.minx + bbox.maxx) / 2\n",
        "        py = (bbox.miny + bbox.maxy) / 2\n",
        "\n",
        "        # Transformaci√≥n Affine: Pixel (col, row) -> Coordenada Real (X, Y)\n",
        "        real_x, real_y = rasterio.transform.xy(transform, py, px, offset='center')\n",
        "\n",
        "        records.append({\n",
        "            \"id\": i,\n",
        "            \"confidence\": round(score, 4),\n",
        "            \"pixel_x\": int(px),\n",
        "            \"pixel_y\": int(py),\n",
        "            \"geometry\": Point(real_x, real_y)\n",
        "        })\n",
        "\n",
        "    # Crear y guardar GeoDataFrame\n",
        "    if records:\n",
        "        gdf = gpd.GeoDataFrame(records, crs=crs)\n",
        "        out_geojson = os.path.join(OUTPUT_DIR, \"detected_palms.geojson\")\n",
        "        gdf.to_file(out_geojson, driver=\"GeoJSON\")\n",
        "        print(f\"üíæ GeoJSON guardado en: {out_geojson}\")\n",
        "\n",
        "        out_csv = os.path.join(OUTPUT_DIR, \"detected_palms.csv\")\n",
        "        pd.DataFrame(records).drop(columns='geometry').to_csv(out_csv, index=False)\n",
        "\n",
        "    print(\"\\nüöÄ PROCESO FINALIZADO CON √âXITO\")\n",
        "\n",
        "# --------------------------------------\n",
        "# ENTRY POINT\n",
        "# --------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSmvI2-YTjWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZArT7smnTjmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrmbxErqTjym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from rasterio.windows import Window\n",
        "# Importaciones espec√≠ficas de Faster R-CNN (Tomadas de test1_2.py)\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.resnet import ResNet101_Weights\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ‚öôÔ∏è CONFIGURATION\n",
        "# ==============================================================================\n",
        "IMAGE_PATH = \"Sample-drone-data-copy.tif\"\n",
        "MODEL_PATH = \"restnet101__exp63_best.pt\" # <--- ¬°Tu archivo .pt de Faster RCNN!\n",
        "OUTPUT_DIR = \"final_results_fasterrcnn\"\n",
        "\n",
        "# INFERENCE SETTINGS\n",
        "SLICE_SIZE = 640         # Must match your training size\n",
        "OVERLAP_RATIO = 0.2      # 20% overlap\n",
        "CONFIDENCE = 0.25        # 25% minimum confidence\n",
        "IOU_THRESHOLD = 0.5      # 50% overlap threshold (NMS)\n",
        "\n",
        "\n",
        "# TILE_SIZE = 800\n",
        "# OVERLAP = 100           # P√≠xeles de solapamiento\n",
        "# CONFIDENCE = 0.65\n",
        "# IOU_THRESHOLD = 0.3\n",
        "\n",
        "# VISUALIZATION SETTINGS\n",
        "DOT_SIZE = 10\n",
        "DOT_COLOR = (0, 0, 255) # Red (BGR)\n",
        "BG_COLOR = (0, 0, 0)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. FUNCI√ìN DE CONSTRUCCI√ìN DEL MODELO (tomada de test1_2.py)\n",
        "# ==============================================================================\n",
        "def get_fasterrcnn_model(num_classes, device, pretrained=False):\n",
        "    \"\"\"Crea la estructura del modelo ResNet101 + FPN y carga los pesos.\"\"\"\n",
        "\n",
        "    backbone_model = resnet_fpn_backbone(\n",
        "        backbone_name='resnet101',\n",
        "        weights=ResNet101_Weights.DEFAULT if pretrained else None,\n",
        "        trainable_layers=5\n",
        "    )\n",
        "\n",
        "    model = FasterRCNN(backbone_model, num_classes=num_classes)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Cargar tus pesos entrenados\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "\n",
        "    model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNCI√ìN DE INFERENCIA PRINCIPAL\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    if not os.path.exists(IMAGE_PATH) or not os.path.exists(MODEL_PATH):\n",
        "        sys.exit(f\"‚ùå Error: Archivo TIFF o modelo .pt no encontrado.\")\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(f\"‚úÖ Usando dispositivo: {device}\")\n",
        "\n",
        "    # --------------------------------------\n",
        "    # 1. CARGAR MODELO Y ABRIR RASTERIO\n",
        "    # --------------------------------------\n",
        "    detection_model = get_fasterrcnn_model(num_classes=2, device=device)\n",
        "    all_detections = []\n",
        "\n",
        "    with rasterio.open(IMAGE_PATH) as src:\n",
        "        transform_base = src.transform\n",
        "        crs = src.crs\n",
        "        img_w, img_h = src.width, src.height\n",
        "\n",
        "        # Leemos la imagen completa para visualizaci√≥n\n",
        "        img_array_vis = src.read([2, 3, 1])\n",
        "\n",
        "        # Conversi√≥n 16-bit a 8-bit para visualizaci√≥n\n",
        "        max_val = np.max(img_array_vis)\n",
        "        if max_val > 255:\n",
        "            img_scaled = img_array_vis.astype(np.float32) * (255.0 / max_val)\n",
        "            img_final_uint8 = img_scaled.astype(np.uint8)\n",
        "        else:\n",
        "            img_final_uint8 = img_array_vis.astype(np.uint8)\n",
        "\n",
        "        img_cv_vis = np.transpose(img_final_uint8, (1, 2, 0))\n",
        "        img_cv_vis = cv2.cvtColor(img_cv_vis, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # --------------------------------------\n",
        "        # 2. LOOP DE TILING MANUAL (EL FIX)\n",
        "        # --------------------------------------\n",
        "        print(f\"‚è≥ Iniciando inferencia en {img_w}x{img_h} con tiles de {TILE_SIZE} y {OVERLAP}px de solape...\")\n",
        "\n",
        "        step = TILE_SIZE - OVERLAP\n",
        "\n",
        "        # Iteraci√≥n manual sobre filas y columnas con el paso definido (step)\n",
        "        for row in range(0, img_h, step):\n",
        "            for col in range(0, img_w, step):\n",
        "\n",
        "                # Definir la ventana de recorte: col_off/row_off indican el inicio\n",
        "                # width/height se ajustan al l√≠mite de la imagen si es el √∫ltimo tile\n",
        "                window = Window(\n",
        "                    col_off=col,\n",
        "                    row_off=row,\n",
        "                    width=min(TILE_SIZE, img_w - col),\n",
        "                    height=min(TILE_SIZE, img_h - row)\n",
        "                )\n",
        "\n",
        "                # Obtener la transformaci√≥n espec√≠fica para este tile\n",
        "                transform = src.window_transform(window)\n",
        "\n",
        "                # Leer el tile: (Canales, Alto, Ancho)\n",
        "                tile_data = src.read(indexes=[2, 3, 1], window=window, boundless=False)\n",
        "\n",
        "                # Aplicar la misma conversi√≥n 16-bit -> 8-bit al tile\n",
        "                tile_max = np.max(tile_data)\n",
        "                if tile_max > 255:\n",
        "                    tile_scaled = tile_data.astype(np.float32) * (255.0 / tile_max)\n",
        "                    tile_final_uint8 = tile_scaled.astype(np.uint8)\n",
        "                else:\n",
        "                    tile_final_uint8 = tile_data.astype(np.uint8)\n",
        "\n",
        "                # Transponer a (H, W, C) y convertir a Tensor (C, H, W) [0, 1]\n",
        "                tile_HWC = np.transpose(tile_final_uint8, (1, 2, 0))\n",
        "                tile_tensor = torch.from_numpy(tile_HWC).float().permute(2, 0, 1) / 255.0\n",
        "\n",
        "                # Agregar dimensi√≥n de batch y mover a GPU\n",
        "                tile_tensor = tile_tensor.to(device).unsqueeze(0)\n",
        "\n",
        "                # --------------------------------------\n",
        "                # 3. INFERENCIA DEL TILE\n",
        "                # --------------------------------------\n",
        "                with torch.no_grad():\n",
        "                    predictions = detection_model(tile_tensor)[0]\n",
        "\n",
        "                # --------------------------------------\n",
        "                # 4. PROCESAR Y GEO-REFERENCIAR\n",
        "                # --------------------------------------\n",
        "                scores = predictions['scores'].cpu().numpy()\n",
        "                boxes = predictions['boxes'].cpu().numpy()\n",
        "\n",
        "                # Filtrar por confianza (opcional, pero √∫til para reducir el trabajo de NMS)\n",
        "                high_conf_indices = scores >= CONFIDENCE\n",
        "                scores = scores[high_conf_indices]\n",
        "                boxes = boxes[high_conf_indices]\n",
        "\n",
        "                # Obtener el desplazamiento global del tile\n",
        "                offset_x = window.col_off\n",
        "                offset_y = window.row_off\n",
        "\n",
        "                for score, box in zip(scores, boxes):\n",
        "                    # 1. Convertir coordenadas del tile (px, py) a coordenadas GLOBALES (col, row)\n",
        "                    box_global = [\n",
        "                        box[0] + offset_x, box[1] + offset_y,\n",
        "                        box[2] + offset_x, box[3] + offset_y\n",
        "                    ]\n",
        "\n",
        "                    # 2. Calcular Centroide Global\n",
        "                    px = (box_global[0] + box_global[2]) / 2  # Columna\n",
        "                    py = (box_global[1] + box_global[3]) / 2  # Fila\n",
        "\n",
        "                    # 3. Geo-referenciar: (col, row) -> (x, y)\n",
        "                    real_x, real_y = rasterio.transform.xy(transform_base, py, px, offset='center')\n",
        "\n",
        "                    all_detections.append({\n",
        "                        \"score\": score,\n",
        "                        \"boxes\": box_global,\n",
        "                        \"pixel_x\": px,\n",
        "                        \"pixel_y\": py,\n",
        "                        \"real_x\": real_x,\n",
        "                        \"real_y\": real_y\n",
        "                    })\n",
        "\n",
        "\n",
        "        # --------------------------------------\n",
        "        # 5. POST-PROCESAMIENTO (NMS GLOBAL)\n",
        "        # --------------------------------------\n",
        "        if not all_detections:\n",
        "            print(\"‚ö†Ô∏è No se detectaron palmas. Proceso finalizado.\")\n",
        "            return\n",
        "\n",
        "        print(\"‚è≥ Aplicando NMS Global para eliminar duplicados del solapamiento...\")\n",
        "        df_all = pd.DataFrame(all_detections)\n",
        "\n",
        "        boxes_tensor = torch.as_tensor(df_all['boxes'].tolist(), dtype=torch.float32)\n",
        "        scores_tensor = torch.as_tensor(df_all['score'].values, dtype=torch.float32)\n",
        "\n",
        "        keep_indices = torchvision.ops.nms(boxes_tensor, scores_tensor, iou_threshold=IOU_THRESHOLD)\n",
        "\n",
        "        final_df = df_all.iloc[keep_indices.cpu().numpy()]\n",
        "        total_palms = len(final_df)\n",
        "        print(f\"‚úÖ Detecciones finales despu√©s de NMS: {total_palms}\")\n",
        "\n",
        "\n",
        "        # --------------------------------------\n",
        "        # 6. EXPORTAR GEOJSON y PNG VISUAL\n",
        "        # --------------------------------------\n",
        "        print(\"üíæ Guardando resultados...\")\n",
        "\n",
        "        # Crear GeoDataFrame\n",
        "        gdf = gpd.GeoDataFrame(\n",
        "            final_df,\n",
        "            geometry=[Point(xy) for xy in zip(final_df.real_x, final_df.real_y)],\n",
        "            crs=crs\n",
        "        )\n",
        "\n",
        "        # 6.1 Guardar GeoJSON\n",
        "        out_geojson = os.path.join(OUTPUT_DIR, \"detected_palms_fasterrcnn.geojson\")\n",
        "        gdf.to_file(out_geojson, driver=\"GeoJSON\")\n",
        "\n",
        "        # 6.2 Guardar PNG Visual\n",
        "        for index, row in final_df.iterrows():\n",
        "            cv2.circle(img_cv_vis, (int(row['pixel_x']), int(row['pixel_y'])), DOT_SIZE, DOT_COLOR, -1)\n",
        "\n",
        "        # Dashboard Label\n",
        "        label_text = f\" Palms: {total_palms} \"\n",
        "        cv2.rectangle(img_cv_vis, (0, 0), (350, 60), BG_COLOR, -1)\n",
        "        cv2.putText(img_cv_vis, label_text, (10, 45),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)\n",
        "\n",
        "        out_png = os.path.join(OUTPUT_DIR, \"mapa_deteccion_fasterrcnn.png\")\n",
        "        cv2.imwrite(out_png, img_cv_vis)\n",
        "\n",
        "        print(f\"\\nüöÄ PROCESO FINALIZADO CON √âXITO.\")\n",
        "        print(f\"Archivo GeoJSON: {out_geojson}\")\n",
        "        print(f\"Imagen Visual:   {out_png}\")\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# FIXED ENTRY POINT\n",
        "# --------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTe0kkJ4Ta76",
        "outputId": "7b88f55e-f241-4984-c623-ae3091d389fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Usando dispositivo: cuda\n",
            "‚è≥ Iniciando inferencia en 9424x5603 con tiles de 800 y 100px de solape...\n",
            "‚è≥ Aplicando NMS Global para eliminar duplicados del solapamiento...\n",
            "‚úÖ Detecciones finales despu√©s de NMS: 8083\n",
            "üíæ Guardando resultados...\n",
            "\n",
            "üöÄ PROCESO FINALIZADO CON √âXITO.\n",
            "Archivo GeoJSON: final_results_fasterrcnn/detected_palms_fasterrcnn.geojson\n",
            "Imagen Visual:   final_results_fasterrcnn/mapa_deteccion_fasterrcnn.png\n"
          ]
        }
      ]
    }
  ]
}